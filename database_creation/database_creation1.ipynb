{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "542c0c6f",
   "metadata": {},
   "source": [
    "## DB Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5156814b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"dataset_instagram-scraper_2025-04-15_20-06-06-456.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b230310e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"samsung_social\"]\n",
    "client.drop_database(\"samsung_social\")\n",
    "\n",
    "\n",
    "\n",
    "posts_col = db[\"posts\"]\n",
    "comments_col = db[\"comments\"]\n",
    "\n",
    "\n",
    "# Optional: Clear collections before reloading\n",
    "posts_col.drop()\n",
    "comments_col.drop()\n",
    "\n",
    "for post in data:\n",
    "    post_doc = {\n",
    "        \"_id\": post[\"id\"],\n",
    "        \"short_code\": post[\"shortCode\"],\n",
    "        \"type\": post[\"type\"],\n",
    "        \"caption\": post.get(\"caption\"),\n",
    "        \"hashtags\": post.get(\"hashtags\", []),\n",
    "        \"url\": post[\"url\"],\n",
    "        \"media_url\": post.get(\"displayUrl\") or post.get(\"videoUrl\"),\n",
    "        \"likes_count\": post.get(\"likesCount\"),\n",
    "        \"comments_count\": post.get(\"commentsCount\"),\n",
    "        \"video_view_count\": post.get(\"videoViewCount\"),\n",
    "        \"timestamp\": post.get(\"timestamp\"),\n",
    "        \"is_sponsored\": post.get(\"isSponsored\", False),\n",
    "        \"product_type\": post.get(\"productType\")\n",
    "    }\n",
    "\n",
    "    # Use upsert to avoid duplication\n",
    "    posts_col.update_one(\n",
    "        { \"_id\": post[\"id\"] },\n",
    "        { \"$set\": post_doc },\n",
    "        upsert=True\n",
    "    )\n",
    "\n",
    "    for comment in post.get(\"latestComments\", []):\n",
    "        user = comment[\"owner\"]\n",
    "\n",
    "        comments_col.update_one(\n",
    "            { \"_id\": comment[\"id\"] },\n",
    "            {\n",
    "                \"$set\": {\n",
    "                    \"post_id\": post[\"id\"],\n",
    "                    \"text\": comment[\"text\"],\n",
    "                    \"timestamp\": comment[\"timestamp\"],\n",
    "                    \"likes_count\": comment[\"likesCount\"],\n",
    "                    \"owner_username\": comment[\"ownerUsername\"],\n",
    "                    \"owner_verified\": user[\"is_verified\"],\n",
    "                    \"owner_profile_pic_url\": user[\"profile_pic_url\"],\n",
    "                    \"owner_id\": user[\"id\"]\n",
    "                }\n",
    "            },\n",
    "            upsert=True\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9f24850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts collection count: 62\n"
     ]
    }
   ],
   "source": [
    "with open(\"dataset2_instagram-scraper_2025-04-17_07-07-37-205.json\") as f:\n",
    "    indirect_data = json.load(f)\n",
    "\n",
    "\n",
    "indirect_col = db[\"indirect_mentions\"]\n",
    "\n",
    "for post in indirect_data:\n",
    "    doc = {\n",
    "        \"_id\": post[\"id\"],\n",
    "        \"source\": \"indirect\",\n",
    "        \"caption\": post.get(\"caption\"),\n",
    "        \"owner_username\": post.get(\"ownerUsername\"),\n",
    "        \"owner_id\": post.get(\"ownerId\"),\n",
    "        \"is_sponsored\": post.get(\"isSponsored\"),\n",
    "        \"location\": post.get(\"locationName\"),\n",
    "        \"likes_count\": post.get(\"likesCount\"),\n",
    "        \"comments_count\": post.get(\"commentsCount\"),\n",
    "        \"url\": post.get(\"url\"),\n",
    "        \"video_url\": post.get(\"videoUrl\"),\n",
    "        \"media_url\": post.get(\"displayUrl\"),\n",
    "        \"product_type\": post.get(\"productType\"),\n",
    "        \"timestamp\": post.get(\"timestamp\"),\n",
    "        \"video_duration\": post.get(\"videoDuration\"),\n",
    "        \"brand_reference\": \"Samsung\"\n",
    "    }\n",
    "    indirect_col.update_one({ \"_id\": doc[\"_id\"] }, { \"$set\": doc }, upsert=True)\n",
    "\n",
    "print(\"Posts collection count:\", posts_col.count_documents({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f97f33",
   "metadata": {},
   "source": [
    "## Sentiment Analysis on Comments and Indirect Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "781b9b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/versaia/Desktop/social_by_design/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comments scored\n",
      "Indirect mentions scored\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.nn.functional import softmax\n",
    "import torch\n",
    "\n",
    "# Load model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "def get_sentiment_score(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        scores = softmax(outputs.logits, dim=1)\n",
    "    stars = torch.argmax(scores) + 1\n",
    "    return stars.item(), scores[0].tolist()\n",
    "  # integer score, full dist\n",
    "\n",
    "# ---- Apply to comments ----\n",
    "for comment in db.comments.find({ \"sentiment_stars\": { \"$exists\": False } }):\n",
    "    text = comment.get(\"text\", \"\")\n",
    "    if not text.strip():\n",
    "        continue\n",
    "\n",
    "    stars, probs = get_sentiment_score(text)\n",
    "\n",
    "    db.comments.update_one(\n",
    "        { \"_id\": comment[\"_id\"] },\n",
    "        {\n",
    "            \"$set\": {\n",
    "                \"sentiment_stars\": stars,\n",
    "                \"sentiment_distribution\": probs\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"Comments scored\")\n",
    "\n",
    "# ---- Apply to indirect posts ----\n",
    "for post in db.indirect_mentions.find({ \"sentiment_stars\": { \"$exists\": False } }):\n",
    "    text = post.get(\"caption\", \"\")\n",
    "    if not text.strip():\n",
    "        continue\n",
    "\n",
    "    stars, probs = get_sentiment_score(text)\n",
    "\n",
    "    db.indirect_mentions.update_one(\n",
    "        { \"_id\": post[\"_id\"] },\n",
    "        {\n",
    "            \"$set\": {\n",
    "                \"sentiment_stars\": stars,\n",
    "                \"sentiment_distribution\": probs\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"Indirect mentions scored\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88848f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment Sentiment (avg): 3.00  from 390 comments\n",
      "Indirect Post Sentiment (avg): 3.42  from 19 posts\n"
     ]
    }
   ],
   "source": [
    "# --- Mean for comments ---\n",
    "pipeline_comments = [\n",
    "    { \"$match\": { \"sentiment_stars\": { \"$exists\": True } } },\n",
    "    { \"$group\": { \"_id\": None, \"avg_sentiment\": { \"$avg\": \"$sentiment_stars\" }, \"count\": { \"$sum\": 1 } } }\n",
    "]\n",
    "comment_result = list(db.comments.aggregate(pipeline_comments))[0]\n",
    "print(f\"Comment Sentiment (avg): {comment_result['avg_sentiment']:.2f}  from {comment_result['count']} comments\")\n",
    "\n",
    "# --- Mean for indirect mentions ---\n",
    "pipeline_mentions = [\n",
    "    { \"$match\": { \"sentiment_stars\": { \"$exists\": True } } },\n",
    "    { \"$group\": { \"_id\": None, \"avg_sentiment\": { \"$avg\": \"$sentiment_stars\" }, \"count\": { \"$sum\": 1 } } }\n",
    "]\n",
    "mention_result = list(db.indirect_mentions.aggregate(pipeline_mentions))[0]\n",
    "print(f\"Indirect Post Sentiment (avg): {mention_result['avg_sentiment']:.2f}  from {mention_result['count']} posts\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca69eff",
   "metadata": {},
   "source": [
    "#### Clean the text for a better topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a75e6079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "import torch\n",
    "\n",
    "# Simple stopwords list since NLTK might cause issues\n",
    "STOPWORDS = {\n",
    "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he',\n",
    "    'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the', 'to', 'was', 'were',\n",
    "    'will', 'with', 'the', 'this', 'but', 'they', 'have', 'had', 'what', 'when',\n",
    "    'where', 'who', 'which', 'why', 'can', 'could', 'should', 'would', 'may',\n",
    "    'might', 'must', 'shall', 'instagram', 'post', 'photo', 'video', 'follow',\n",
    "    'like', 'comment', 'share', 'samsung', 'galaxy', 'phone', 'smartphone',\n",
    "    'via', 'amp', 'rt', 'http', 'https', 'www', 'com', 'co', 'new', 'day',\n",
    "    'today', 'get', 'got', 'im', 'one', 'going', 'want', 'know', 'time',\n",
    "    'really', 'see', 'going', 'u', 'ur', 'dont', 'thats', 'cant', 'ive',\n",
    "    'im', 'youre', 'didnt', 'wont', 'isnt', 'arent', 'wasnt', 'werent'\n",
    "}\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Enhanced text cleaning function\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove mentions and hashtags\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    # Remove emojis and special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenize and remove stopwords\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in STOPWORDS and len(word) > 2]\n",
    "    # Rejoin the text\n",
    "    text = ' '.join(words)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eeea6a",
   "metadata": {},
   "source": [
    "## Topic Modelling for Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7de0cf87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing comments...\n",
      "Found 296 valid comments after cleaning\n",
      "Loading embedding model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2025-04-18 09:45:07,059 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting topic model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 10/10 [00:11<00:00,  1.11s/it]\n",
      "2025-04-18 09:45:18,179 - BERTopic - Embedding - Completed ✓\n",
      "2025-04-18 09:45:18,180 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-04-18 09:45:25,413 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-18 09:45:25,414 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-04-18 09:45:25,435 - BERTopic - Cluster - Completed ✓\n",
      "2025-04-18 09:45:25,438 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-04-18 09:45:25,509 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating database...\n",
      "\n",
      "Topic Analysis Summary:\n",
      "    Topic  Count                                     Name  \\\n",
      "0      -1     19             -1_music_inches_company_star   \n",
      "1       0     18              0_apple_лучшие_iphone_about   \n",
      "2       1      8          1_compren_hacen_pésimo_servicio   \n",
      "3       2      8            2_kami_belum_pengguna_upgrade   \n",
      "4       3      8                    3_weak_vfx_omgggg_nhl   \n",
      "5       4      8            4_release_oneui_ashamed_april   \n",
      "6       5      7       5_cool_kitchen_refrigerator_fridge   \n",
      "7       6      7          6_monitor_pixel_experience_dead   \n",
      "8       7      7            7_washing_machines_repair_how   \n",
      "9       8      7                 8_días_con_televisor_que   \n",
      "10      9      7          9_running_wheres_everywhere_ceo   \n",
      "11     10      7         10_dishwasher_came_spare_charged   \n",
      "12     11      7          11_nice_referring_yes_marketing   \n",
      "13     12      6          12_ayıplı_comments_messages_mal   \n",
      "14     13      6         13_ordered_delivery_model_refund   \n",
      "15     14      6             14_سامسونگ_گوشی_برند_stripes   \n",
      "16     15      6           15_seamless_mix_enzos_adorable   \n",
      "17     16      6          16_beautiful_love_llega_outfits   \n",
      "18     17      5                  17_free_hai_mobile_dekh   \n",
      "19     18      5                 18_einstein_oui_oni_yeah   \n",
      "20     19      5                19_você_uncle_reply_iphon   \n",
      "21     20      5                20_android_شركه_اتصرف_بيه   \n",
      "22     21      5               21_wow_great_awesome_super   \n",
      "23     22      5           22_warranty_claim_brand_screen   \n",
      "24     23      5               23_gün_acaba_sevgili_misin   \n",
      "25     24      5          24_refrigerador_sin_una_semanas   \n",
      "26     25      5                25_feo_quantum_jokes_feio   \n",
      "27     26      4             26_hear_tvs_reminder_garbage   \n",
      "28     27      4             27_geladeira_não_زفت_comprei   \n",
      "29     28      4                    28_perfect_birthday__   \n",
      "30     29      4           29_year_products_mejor_pésimos   \n",
      "31     30      4            30_their_leaders_world_people   \n",
      "32     31      4         31_number_meaning_contacted_vlog   \n",
      "33     32      4     32_brazil_comprar_tell____reclamação   \n",
      "34     33      4                   33_دونه_steak_güh_عاشق   \n",
      "35     34      4          34_device_problem_chronic_strip   \n",
      "36     35      4                    35_pakek_use_gak_fold   \n",
      "37     36      4          36_gods_teacher_hospital_angels   \n",
      "38     37      4                37_watch_type_table_couch   \n",
      "39     38      4             38_jajaja_ubiera_shuga_salir   \n",
      "40     39      4        39_malfunction_imma_therez_broken   \n",
      "41     40      4                        40_suga_fuck_ass_   \n",
      "42     41      4          41_trabajo_quisiera_para_quiere   \n",
      "43     42      3       42_condicionado_design_class_apart   \n",
      "44     43      3                  43_women_mom_mali_loves   \n",
      "45     44      3           44_smartest_presta_mercado_try   \n",
      "46     45      3          45_tablet_estão_pessoas_receber   \n",
      "47     46      3           46_era_suddenly_problema_first   \n",
      "48     47      3           47_complaints_gifts_people_air   \n",
      "49     48      3          48_access_internet_forget_dessa   \n",
      "50     49      3           49_ama_recomendo_defeito_nunca   \n",
      "51     50      3     50_plus_divest_boycott_antiimmigrant   \n",
      "52     51      3               51_ballie_playmate_his_hey   \n",
      "53     52      3       52_notebook_esse_bateria_atualizar   \n",
      "54     53      3           53_you_complaining_spare_first   \n",
      "55     54      3  54_appear_automatically_public_pathetic   \n",
      "\n",
      "                                       Representation  \\\n",
      "0   [music, inches, company, star, production, siz...   \n",
      "1   [apple, лучшие, iphone, about, шикарные, самые...   \n",
      "2   [compren, hacen, pésimo, servicio, línea, acla...   \n",
      "3   [kami, belum, pengguna, upgrade, nikmati, seka...   \n",
      "4   [weak, vfx, omgggg, nhl, kxkg, gnngxh, f_cking...   \n",
      "5   [release, oneui, ashamed, april, yourself, upd...   \n",
      "6   [cool, kitchen, refrigerator, fridge, cares, w...   \n",
      "7   [monitor, pixel, experience, dead, seriously, ...   \n",
      "8   [washing, machines, repair, how, oil, floors, ...   \n",
      "9   [días, con, televisor, que, llegó, defectuoso,...   \n",
      "10  [running, wheres, everywhere, ceo, possible, m...   \n",
      "11  [dishwasher, came, spare, charged, service, st...   \n",
      "12  [nice, referring, yes, marketing, aahh, bro, b...   \n",
      "13  [ayıplı, comments, messages, mal, losing, poss...   \n",
      "14  [ordered, delivery, model, refund, calls, appl...   \n",
      "15  [سامسونگ, گوشی, برند, stripes, phones, آیفون, ...   \n",
      "16  [seamless, mix, enzos, adorable, ones, all, lo...   \n",
      "17  [beautiful, love, llega, outfits, matching, pa...   \n",
      "18  [free, hai, mobile, dekh, kyu, fire, favor, pa...   \n",
      "19  [einstein, oui, oni, yeah, cadê, love, just, ,...   \n",
      "20  [você, uncle, reply, iphon, findme, well, your...   \n",
      "21  [android, شركه, اتصرف, بيه, حتي, تقولك, اللي, ...   \n",
      "22           [wow, great, awesome, super, , , , , , ]   \n",
      "23  [warranty, claim, brand, screen, waiting, week...   \n",
      "24  [gün, acaba, sevgili, misin, eder, flip, hediy...   \n",
      "25  [refrigerador, sin, una, semanas, estafa, comi...   \n",
      "26  [feo, quantum, jokes, feio, dots, bagulho, hit...   \n",
      "27  [hear, tvs, reminder, garbage, needs, absolute...   \n",
      "28  [geladeira, não, زفت, comprei, nada, يعملوا, ع...   \n",
      "29                [perfect, birthday, , , , , , , , ]   \n",
      "30  [year, products, mejor, pésimos, induction, ev...   \n",
      "31  [their, leaders, world, people, countries, jew...   \n",
      "32  [number, meaning, contacted, vlog, chase, trac...   \n",
      "33  [brazil, comprar, tell, ___reclamação, guarant...   \n",
      "34  [دونه, steak, güh, عاشق, ask, pieces, notem, d...   \n",
      "35  [device, problem, chronic, strip, green, note,...   \n",
      "36  [pakek, use, gak, fold, annoyed, macro, wide, ...   \n",
      "37  [gods, teacher, hospital, angels, clinic, eter...   \n",
      "38  [watch, type, table, couch, ultra, best, waiti...   \n",
      "39  [jajaja, ubiera, shuga, salir, quien, 아트티비, pr...   \n",
      "40  [malfunction, imma, therez, broken, fix, broke...   \n",
      "41                    [suga, fuck, ass, , , , , , , ]   \n",
      "42  [trabajo, quisiera, para, quiere, niña, ayuda,...   \n",
      "43   [condicionado, design, class, apart, , , , , , ]   \n",
      "44    [women, mom, mali, loves, ela, team, pra, , , ]   \n",
      "45  [smartest, presta, mercado, try, decision, pio...   \n",
      "46  [tablet, estão, pessoas, receber, troubled, sé...   \n",
      "47  [era, suddenly, problema, first, abri, mensage...   \n",
      "48  [complaints, gifts, people, air, microwaves, e...   \n",
      "49  [access, internet, forget, dessa, cannot, lixo...   \n",
      "50  [ama, recomendo, defeito, nunca, hediye, compr...   \n",
      "51  [plus, divest, boycott, antiimmigrant, commerc...   \n",
      "52  [ballie, playmate, his, hey, hes, ball, treati...   \n",
      "53  [notebook, esse, bateria, atualizar, problema,...   \n",
      "54  [you, complaining, spare, first, service, mont...   \n",
      "55  [appear, automatically, public, pathetic, awar...   \n",
      "\n",
      "                                  Representative_Docs  \n",
      "0   [meu aparelho está mais mês assistência técnic...  \n",
      "1   [about apple products, apple, все знают что эк...  \n",
      "2   [compren línea tienen peor servicio cliente ha...  \n",
      "3   [позор самсунг могут сделать нормально обновле...  \n",
      "4             [gnngxh, kxkg, weak vfx better contact]  \n",
      "5   [did take long release oneui release everyone ...  \n",
      "6   [cool fridge, such cool fridge you guys best f...  \n",
      "7   [recently bought monitor within found dead pix...  \n",
      "8   [washing machines not buy customer service sho...  \n",
      "9   [bro bought monitor march within just days dev...  \n",
      "10     [wheres, only possible, everywhere everything]  \n",
      "11  [very poor quality aftersales service service ...  \n",
      "12                                 [nice, nice, nice]  \n",
      "13  [technical support shitty sending messages ask...  \n",
      "14  [absolute worst customer service wish paid mor...  \n",
      "15  [چرتترین برند گوشی آیفون سامسونگ, bought cell ...  \n",
      "16  [whos mix ones puppy maybe never notice all cu...  \n",
      "17  [hermoso para cuando llega ciudad méxico, yess...  \n",
      "18  [you something hang mobile waist, mobile hangi...  \n",
      "19           [cadê oni, einstein, just love einstein]  \n",
      "20    [you findme, wait your reply, iphon your uncle]  \n",
      "21  [roll out android pls, شركه سامسونج بالفعل اسو...  \n",
      "22                                  [great, wow, wow]  \n",
      "23  [brand screen broken few days weeks laterstill...  \n",
      "24  [sevgili flip hediye eder misin acaba gün, sev...  \n",
      "25  [fridges trash please not buy year old door be...  \n",
      "26  [que bagulho feio, يخوي لتحاولون تصير مثل التي...  \n",
      "27  [ley del planeta televisor esta hackeando más ...  \n",
      "28  [comprei uma máquina veio toda amassada não qu...  \n",
      "29                        [perfect, perfect, perfect]  \n",
      "30  [buy defective batch everyone complaining abou...  \n",
      "31  [los contacte infinidad veces sin respuesta pa...  \n",
      "32  [fish missing just your after sales service, w...  \n",
      "33  [brazil worst technical support possible brazi...  \n",
      "34  [notem que meses atrás tentei contato vocês, m...  \n",
      "35  [after system update note ultra displayed gree...  \n",
      "36  [gak mau lagi pakek fold bulan maret kmrn abis...  \n",
      "37  [شركه كويسه نصبوا علينا وخدوا فلوس تصليح الغسا...  \n",
      "38  [best watch table, watch ultra, waiting watch ...  \n",
      "39  [아트티비, que salir gta primero jajaja quien ubie...  \n",
      "40       [broken, therez malfunction, imma broke buy]  \n",
      "41                                  [ass, suga, suga]  \n",
      "42  [need, need these life, quisiera pedir ayuda q...  \n",
      "43   [condicionado, class design apart, condicionado]  \n",
      "44              [pra ela, mom loves, mali women team]  \n",
      "45  [never used let try, smartest decision you mak...  \n",
      "46  [very much you giant global company solve prob...  \n",
      "47  [essa mensagem entrada úmida dor cabeça viu nã...  \n",
      "48  [pior que nem respondem nem ligam para reputaç...  \n",
      "49  [not buy, cannot access internet now forget ab...  \n",
      "50  [comprei minha seis meses uso defeito até agor...  \n",
      "51  [seeing antiimmigrant commercial plus need boy...  \n",
      "52  [ballie back, hey ballie, hes treating ball hi...  \n",
      "53  [vocês negaram reparo meu produto sendo que fo...  \n",
      "54  [very poor quality aftersales service service ...  \n",
      "55  [public awareness green line automatically app...  \n",
      "\n",
      "Detailed Topics:\n",
      "\n",
      "Topic 0 (Count: 18):\n",
      "Keywords: apple, лучшие, iphone, about, шикарные\n",
      "\n",
      "Topic 1 (Count: 8):\n",
      "Keywords: compren, hacen, pésimo, servicio, línea\n",
      "\n",
      "Topic 2 (Count: 8):\n",
      "Keywords: kami, belum, pengguna, upgrade, nikmati\n",
      "\n",
      "Topic 3 (Count: 8):\n",
      "Keywords: weak, vfx, omgggg, nhl, kxkg\n",
      "\n",
      "Topic 4 (Count: 8):\n",
      "Keywords: release, oneui, ashamed, april, yourself\n",
      "\n",
      "Topic 5 (Count: 7):\n",
      "Keywords: cool, kitchen, refrigerator, fridge, cares\n",
      "\n",
      "Topic 6 (Count: 7):\n",
      "Keywords: monitor, pixel, experience, dead, seriously\n",
      "\n",
      "Topic 7 (Count: 7):\n",
      "Keywords: washing, machines, repair, how, oil\n",
      "\n",
      "Topic 8 (Count: 7):\n",
      "Keywords: días, con, televisor, que, llegó\n",
      "\n",
      "Topic 9 (Count: 7):\n",
      "Keywords: running, wheres, everywhere, ceo, possible\n",
      "\n",
      "Topic 10 (Count: 7):\n",
      "Keywords: dishwasher, came, spare, charged, service\n",
      "\n",
      "Topic 11 (Count: 7):\n",
      "Keywords: nice, referring, yes, marketing, aahh\n",
      "\n",
      "Topic 12 (Count: 6):\n",
      "Keywords: ayıplı, comments, messages, mal, losing\n",
      "\n",
      "Topic 13 (Count: 6):\n",
      "Keywords: ordered, delivery, model, refund, calls\n",
      "\n",
      "Topic 14 (Count: 6):\n",
      "Keywords: سامسونگ, گوشی, برند, stripes, phones\n",
      "\n",
      "Topic 15 (Count: 6):\n",
      "Keywords: seamless, mix, enzos, adorable, ones\n",
      "\n",
      "Topic 16 (Count: 6):\n",
      "Keywords: beautiful, love, llega, outfits, matching\n",
      "\n",
      "Topic 17 (Count: 5):\n",
      "Keywords: free, hai, mobile, dekh, kyu\n",
      "\n",
      "Topic 18 (Count: 5):\n",
      "Keywords: einstein, oui, oni, yeah, cadê\n",
      "\n",
      "Topic 19 (Count: 5):\n",
      "Keywords: você, uncle, reply, iphon, findme\n",
      "\n",
      "Topic 20 (Count: 5):\n",
      "Keywords: android, شركه, اتصرف, بيه, حتي\n",
      "\n",
      "Topic 21 (Count: 5):\n",
      "Keywords: wow, great, awesome, super, \n",
      "\n",
      "Topic 22 (Count: 5):\n",
      "Keywords: warranty, claim, brand, screen, waiting\n",
      "\n",
      "Topic 23 (Count: 5):\n",
      "Keywords: gün, acaba, sevgili, misin, eder\n",
      "\n",
      "Topic 24 (Count: 5):\n",
      "Keywords: refrigerador, sin, una, semanas, estafa\n",
      "\n",
      "Topic 25 (Count: 5):\n",
      "Keywords: feo, quantum, jokes, feio, dots\n",
      "\n",
      "Topic 26 (Count: 4):\n",
      "Keywords: hear, tvs, reminder, garbage, needs\n",
      "\n",
      "Topic 27 (Count: 4):\n",
      "Keywords: geladeira, não, زفت, comprei, nada\n",
      "\n",
      "Topic 28 (Count: 4):\n",
      "Keywords: perfect, birthday, , , \n",
      "\n",
      "Topic 29 (Count: 4):\n",
      "Keywords: year, products, mejor, pésimos, induction\n",
      "\n",
      "Topic 30 (Count: 4):\n",
      "Keywords: their, leaders, world, people, countries\n",
      "\n",
      "Topic 31 (Count: 4):\n",
      "Keywords: number, meaning, contacted, vlog, chase\n",
      "\n",
      "Topic 32 (Count: 4):\n",
      "Keywords: brazil, comprar, tell, ___reclamação, guaranteed\n",
      "\n",
      "Topic 33 (Count: 4):\n",
      "Keywords: دونه, steak, güh, عاشق, ask\n",
      "\n",
      "Topic 34 (Count: 4):\n",
      "Keywords: device, problem, chronic, strip, green\n",
      "\n",
      "Topic 35 (Count: 4):\n",
      "Keywords: pakek, use, gak, fold, annoyed\n",
      "\n",
      "Topic 36 (Count: 4):\n",
      "Keywords: gods, teacher, hospital, angels, clinic\n",
      "\n",
      "Topic 37 (Count: 4):\n",
      "Keywords: watch, type, table, couch, ultra\n",
      "\n",
      "Topic 38 (Count: 4):\n",
      "Keywords: jajaja, ubiera, shuga, salir, quien\n",
      "\n",
      "Topic 39 (Count: 4):\n",
      "Keywords: malfunction, imma, therez, broken, fix\n",
      "\n",
      "Topic 40 (Count: 4):\n",
      "Keywords: suga, fuck, ass, , \n",
      "\n",
      "Topic 41 (Count: 4):\n",
      "Keywords: trabajo, quisiera, para, quiere, niña\n",
      "\n",
      "Topic 42 (Count: 3):\n",
      "Keywords: condicionado, design, class, apart, \n",
      "\n",
      "Topic 43 (Count: 3):\n",
      "Keywords: women, mom, mali, loves, ela\n",
      "\n",
      "Topic 44 (Count: 3):\n",
      "Keywords: smartest, presta, mercado, try, decision\n",
      "\n",
      "Topic 45 (Count: 3):\n",
      "Keywords: tablet, estão, pessoas, receber, troubled\n",
      "\n",
      "Topic 46 (Count: 3):\n",
      "Keywords: era, suddenly, problema, first, abri\n",
      "\n",
      "Topic 47 (Count: 3):\n",
      "Keywords: complaints, gifts, people, air, microwaves\n",
      "\n",
      "Topic 48 (Count: 3):\n",
      "Keywords: access, internet, forget, dessa, cannot\n",
      "\n",
      "Topic 49 (Count: 3):\n",
      "Keywords: ama, recomendo, defeito, nunca, hediye\n",
      "\n",
      "Topic 50 (Count: 3):\n",
      "Keywords: plus, divest, boycott, antiimmigrant, commercial\n",
      "\n",
      "Topic 51 (Count: 3):\n",
      "Keywords: ballie, playmate, his, hey, hes\n",
      "\n",
      "Topic 52 (Count: 3):\n",
      "Keywords: notebook, esse, bateria, atualizar, problema\n",
      "\n",
      "Topic 53 (Count: 3):\n",
      "Keywords: you, complaining, spare, first, service\n",
      "\n",
      "Topic 54 (Count: 3):\n",
      "Keywords: appear, automatically, public, pathetic, awareness\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    # Process comments\n",
    "    print(\"Processing comments...\")\n",
    "    comment_docs = list(db.comments.find({ \"text\": { \"$exists\": True } }))\n",
    "    filtered_comment_docs = [doc for doc in comment_docs if doc[\"text\"].strip()]\n",
    "    comment_texts = [clean_text(doc[\"text\"]) for doc in filtered_comment_docs]\n",
    "    comment_texts = [text for text in comment_texts if text.strip()]\n",
    "\n",
    "    print(f\"Found {len(comment_texts)} valid comments after cleaning\")\n",
    "\n",
    "    if len(comment_texts) >= 5:\n",
    "        # Load the embedding model\n",
    "        print(\"Loading embedding model...\")\n",
    "        embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "        # Configure UMAP with minimal parameters\n",
    "        umap_model = UMAP(\n",
    "            n_components=2,\n",
    "            n_neighbors=3,\n",
    "            min_dist=0.0,\n",
    "            metric='cosine',\n",
    "            random_state=42,\n",
    "            init='random'\n",
    "        )\n",
    "\n",
    "        # Configure BERTopic with minimal parameters\n",
    "        topic_model = BERTopic(\n",
    "            embedding_model=embedding_model,\n",
    "            umap_model=umap_model,\n",
    "            min_topic_size=2,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        print(\"Fitting topic model...\")\n",
    "        topics, probs = topic_model.fit_transform(comment_texts)\n",
    "\n",
    "        # Update database\n",
    "        print(\"Updating database...\")\n",
    "        for i, (doc, text) in enumerate(zip(filtered_comment_docs, comment_texts)):\n",
    "            if not text:  # Skip empty texts\n",
    "                continue\n",
    "                \n",
    "            topic_id = int(topics[i])\n",
    "            topic_words = topic_model.get_topic(topic_id)\n",
    "            \n",
    "            update_data = {\n",
    "                \"topic_id\": topic_id,\n",
    "                \"topic_words\": [word for word, _ in topic_words[:5]] if topic_words else []\n",
    "            }\n",
    "            \n",
    "            db.comments.update_one(\n",
    "                { \"_id\": doc[\"_id\"] },\n",
    "                { \"$set\": update_data }\n",
    "            )\n",
    "\n",
    "        # Print summary\n",
    "        print(\"\\nTopic Analysis Summary:\")\n",
    "        topic_info = topic_model.get_topic_info()\n",
    "        print(topic_info)\n",
    "        \n",
    "        # Print topics\n",
    "        print(\"\\nDetailed Topics:\")\n",
    "        for topic in topic_info.itertuples():\n",
    "            if topic.Topic != -1:  # Skip outlier topic\n",
    "                print(f\"\\nTopic {topic.Topic} (Count: {topic.Count}):\")\n",
    "                words = [word for word, _ in topic_model.get_topic(topic.Topic)[:5]]\n",
    "                print(\"Keywords:\", \", \".join(words))\n",
    "\n",
    "    else:\n",
    "        print(\"Not enough comments for meaningful topic modeling\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "607267ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating database with topic names...\n",
      "Database updated with topic names.\n"
     ]
    }
   ],
   "source": [
    "# Update database with topic names instead of topic indices\n",
    "print(\"Updating database with topic names...\")\n",
    "for i, (doc, text) in enumerate(zip(filtered_comment_docs, comment_texts)):\n",
    "    if not text:  # Skip empty texts\n",
    "        continue\n",
    "\n",
    "    topic_id = int(topics[i])\n",
    "    topic_words = topic_model.get_topic(topic_id)\n",
    "    topic_name = \", \".join([word for word, _ in topic_words[:5]]) if topic_words else \"Unknown\"\n",
    "\n",
    "    update_data = {\n",
    "        \"topic_name\": topic_name\n",
    "    }\n",
    "\n",
    "    db.comments.update_one(\n",
    "        { \"_id\": doc[\"_id\"] },\n",
    "        { \"$set\": update_data }\n",
    "    )\n",
    "\n",
    "print(\"Database updated with topic names.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122b867d",
   "metadata": {},
   "source": [
    "## Topic Modelling for Indirect Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8191b86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 09:45:25,866 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing mentions...\n",
      "Found 18 valid mentions after cleaning\n",
      "Fitting mention topic model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it]\n",
      "2025-04-18 09:45:26,924 - BERTopic - Embedding - Completed ✓\n",
      "2025-04-18 09:45:26,925 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-04-18 09:45:26,945 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-18 09:45:26,946 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-04-18 09:45:26,950 - BERTopic - Cluster - Completed ✓\n",
      "2025-04-18 09:45:26,953 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-04-18 09:45:26,967 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating mentions in database...\n",
      "\n",
      "Mention Topic Analysis Summary:\n",
      "   Topic  Count                          Name  \\\n",
      "0      0      9  0_setup_color_camera_scanner   \n",
      "1      1      6          1_ᵗʰᵉ_ᵃⁿᵈ_ʸᵒᵘʳ_ᶜʳᵃᶠᵗ   \n",
      "2      2      3      2_درباره_vision_حالا_بود   \n",
      "\n",
      "                                      Representation  \\\n",
      "0  [setup, color, camera, scanner, ich, iso, tech...   \n",
      "1  [ᵗʰᵉ, ᵃⁿᵈ, ʸᵒᵘʳ, ᶜʳᵃᶠᵗ, ᶠᵒʳ, ᵁᵗᵃ, ᶜˡᵃˢˢ, ᵇᵒᵒᵏ,...   \n",
      "2  [درباره, vision, حالا, بود, برنامهها, انگشتان,...   \n",
      "\n",
      "                                 Representative_Docs  \n",
      "0  [camera revue super zoom film agfaphoto color ...  \n",
      "1  [problem hres informаtiоn аbоut mеrcеdеs clr g...  \n",
      "2  [excellence now obsession freedom, folkestone ...  \n"
     ]
    }
   ],
   "source": [
    "# Process mentions\n",
    "print(\"Processing mentions...\")\n",
    "mention_docs = list(db.indirect_mentions.find({ \"caption\": { \"$exists\": True } }))\n",
    "filtered_mention_docs = [doc for doc in mention_docs if doc[\"caption\"].strip()]\n",
    "mention_texts = [clean_text(doc[\"caption\"]) for doc in filtered_mention_docs]\n",
    "mention_texts = [text for text in mention_texts if text.strip()]\n",
    "\n",
    "print(f\"Found {len(mention_texts)} valid mentions after cleaning\")\n",
    "\n",
    "if len(mention_texts) >= 5:\n",
    "    print(\"Fitting mention topic model...\")\n",
    "    topics, probs = topic_model.fit_transform(mention_texts)\n",
    "\n",
    "    print(\"Updating mentions in database...\")\n",
    "    for i, (doc, text) in enumerate(zip(filtered_mention_docs, mention_texts)):\n",
    "        if not text:\n",
    "            continue\n",
    "        \n",
    "        topic_id = int(topics[i])\n",
    "        topic_words = topic_model.get_topic(topic_id)\n",
    "\n",
    "        update_data = {\n",
    "            \"topic_id\": topic_id,\n",
    "            \"topic_words\": [word for word, _ in topic_words[:5]] if topic_words else []\n",
    "        }\n",
    "\n",
    "        db.indirect_mentions.update_one(\n",
    "            { \"_id\": doc[\"_id\"] },\n",
    "            { \"$set\": update_data }\n",
    "        )\n",
    "\n",
    "    print(\"\\nMention Topic Analysis Summary:\")\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    print(topic_info)\n",
    "else:\n",
    "    print(\"Not enough mentions for meaningful topic modeling\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44cd026",
   "metadata": {},
   "source": [
    "## Topic Modelling for brand's own posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd635322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching brand posts...\n",
      "\n",
      "Content Type Distribution:\n",
      "Video: 60 posts\n",
      "Sidecar: 2 posts\n",
      "\n",
      "Processing 62 posts for topic modeling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 09:45:33,465 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|██████████| 2/2 [00:09<00:00,  4.61s/it]\n",
      "2025-04-18 09:45:42,695 - BERTopic - Embedding - Completed ✓\n",
      "2025-04-18 09:45:42,695 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-04-18 09:45:42,745 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-18 09:45:42,746 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-04-18 09:45:42,754 - BERTopic - Cluster - Completed ✓\n",
      "2025-04-18 09:45:42,755 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-04-18 09:45:42,786 - BERTopic - Representation - Completed ✓\n",
      "2025-04-18 09:45:42,787 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-04-18 09:45:42,797 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-04-18 09:45:42,812 - BERTopic - Representation - Completed ✓\n",
      "2025-04-18 09:45:42,814 - BERTopic - Topic reduction - Reduced number of topics from 15 to 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updating posts with topic information...\n",
      "\n",
      "Topic Analysis Summary:\n",
      "   Topic  Count                                     Name  \\\n",
      "0     -1      1      -1_weens_thanks_take_samsungbespoke   \n",
      "1      0     24                 0_art_ces_neoqled_vision   \n",
      "2      1     22  1_home_bespoke_bespokeai_samsungbespoke   \n",
      "3      2      5   2_works_windfree_you_bespokeaiwindfree   \n",
      "4      3      4     3_register_welcome_bespoke_bespokeai   \n",
      "5      4      3                 4_qled_pranks_real_safer   \n",
      "6      5      3                 5_jet_bot_cleaning_clean   \n",
      "\n",
      "                                      Representation  \\\n",
      "0  [weens, thanks, take, samsungbespoke, bespokea...   \n",
      "1  [art, ces, neoqled, vision, samsungces, microl...   \n",
      "2  [home, bespoke, bespokeai, samsungbespoke, you...   \n",
      "3  [works, windfree, you, bespokeaiwindfree, felt...   \n",
      "4  [register, welcome, bespoke, bespokeai, , , , ...   \n",
      "5  [qled, pranks, real, safer, april, quantum, fo...   \n",
      "6  [jet, bot, cleaning, clean, combo, samsungpart...   \n",
      "\n",
      "                                 Representative_Docs  \n",
      "0       [weens take thanks bespokeai samsungbespoke]  \n",
      "1  [unveiling our artnership ces stay tuned lates...  \n",
      "2  [bespoke home living made simple bespoke induc...  \n",
      "3  [you ever felt bespoke windfree isn just air c...  \n",
      "4  [welcome bespoke register bespoke bespokeai, w...  \n",
      "5  [don fall fake especially april fool without q...  \n",
      "6  [jet bot combo saves lots cleaning messes spen...  \n",
      "\n",
      "Detailed Topics and Example Posts:\n",
      "\n",
      "Topic 0 (Count: 24):\n",
      "Keywords: art, ces, neoqled, vision, samsungces\n",
      "Example content:\n",
      "- turn however you series soundbar qsf punches incredible surround sound matter how you place qseriess...\n",
      "- odyssey oled your gaming performance powerhouse oled brings visual clarity take your gaming above be...\n",
      "\n",
      "Topic 1 (Count: 22):\n",
      "Keywords: home, bespoke, bespokeai, samsungbespoke, your\n",
      "Example content:\n",
      "- how seamless integration between bespoke home appliances smartthings app revolutionize your living e...\n",
      "- how seamless integration between bespoke home appliances smartthings app revolutionize your living e...\n",
      "\n",
      "Topic 2 (Count: 5):\n",
      "Keywords: works, windfree, you, bespokeaiwindfree, felt\n",
      "Example content:\n",
      "- you ever felt bespoke windfree isn just air conditioner works well works you effortlessly intuitivel...\n",
      "- you ever felt bespoke windfree isn just air conditioner works well works you effortlessly intuitivel...\n",
      "\n",
      "Topic 3 (Count: 4):\n",
      "Keywords: register, welcome, bespoke, bespokeai, \n",
      "Example content:\n",
      "- welcome bespoke register bespoke bespokeai...\n",
      "- welcome bespoke register bespoke bespokeai...\n",
      "\n",
      "Topic 4 (Count: 3):\n",
      "Keywords: qled, pranks, real, safer, april\n",
      "Example content:\n",
      "- pranks over here april fool real qled quantum dot sheet real thing safer cadmium extra secure knox p...\n",
      "- april fool keeping real qled only qled quantum dot sheet built right also safer cadmium extra secure...\n",
      "\n",
      "Topic 5 (Count: 3):\n",
      "Keywords: jet, bot, cleaning, clean, combo\n",
      "Example content:\n",
      "- bespoke jet bot combo smart shih tzu using object recognition all hard clean areas don usually avoid...\n",
      "- jet bot combo saves lots cleaning messes spend enzy mean how cool always coming most advanced smart ...\n",
      "\n",
      "Topic visualization has been generated.\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Enhanced stopwords for brand content\n",
    "BRAND_STOPWORDS = {\n",
    "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he',\n",
    "    'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the', 'to', 'was', 'were',\n",
    "    'will', 'with', 'the', 'this', 'but', 'they', 'have', 'had', 'what', 'when',\n",
    "    'where', 'who', 'which', 'why', 'can', 'could', 'should', 'would', 'may',\n",
    "    'might', 'must', 'shall', 'instagram', 'post', 'photo', 'video', 'follow',\n",
    "    'like', 'comment', 'share', 'samsung', 'galaxy', 'phone', 'smartphone',\n",
    "    'via', 'amp', 'rt', 'http', 'https', 'www', 'com', 'co', 'new', 'day',\n",
    "    'today', 'get', 'got', 'im', 'one', 'going', 'want', 'know', 'time',\n",
    "    'really', 'see', 'going', 'click', 'link', 'bio', 'learn', 'more',\n",
    "    'discover', 'introducing', 'available', 'now', 'check', 'out'\n",
    "}\n",
    "\n",
    "def clean_brand_text(text):\n",
    "    \"\"\"Enhanced text cleaning function for brand content\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove mentions and hashtags but keep the text\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)  # Keep hashtag content without #\n",
    "    \n",
    "    # Remove emojis and special characters\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Tokenize and remove stopwords\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in BRAND_STOPWORDS and len(word) > 2]\n",
    "    \n",
    "    # Rejoin the text\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "try:\n",
    "    # Fetch all brand posts\n",
    "    print(\"Fetching brand posts...\")\n",
    "    posts = list(db.posts.find())\n",
    "    \n",
    "    # Prepare content for analysis\n",
    "    post_texts = []\n",
    "    post_ids = []\n",
    "    post_types = defaultdict(int)\n",
    "    \n",
    "    for post in posts:\n",
    "        # Combine caption and type information\n",
    "        caption = post.get('caption', '')\n",
    "        post_type = post.get('type', 'unknown')\n",
    "        post_types[post_type] += 1\n",
    "        \n",
    "        # Create enhanced text combining type and caption\n",
    "        combined_text = f\"{post_type} {caption}\"\n",
    "        cleaned_text = clean_brand_text(combined_text)\n",
    "        \n",
    "        if cleaned_text.strip():\n",
    "            post_texts.append(cleaned_text)\n",
    "            post_ids.append(post['_id'])\n",
    "    \n",
    "    print(f\"\\nContent Type Distribution:\")\n",
    "    for ptype, count in post_types.items():\n",
    "        print(f\"{ptype}: {count} posts\")\n",
    "    \n",
    "    if len(post_texts) >= 5:\n",
    "        print(f\"\\nProcessing {len(post_texts)} posts for topic modeling...\")\n",
    "        \n",
    "        # Initialize models\n",
    "        embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "        \n",
    "        umap_model = UMAP(\n",
    "            n_components=2,\n",
    "            n_neighbors=3,\n",
    "            min_dist=0.0,\n",
    "            metric='cosine',\n",
    "            random_state=42,\n",
    "            init='random'\n",
    "        )\n",
    "        \n",
    "        topic_model = BERTopic(\n",
    "            embedding_model=embedding_model,\n",
    "            umap_model=umap_model,\n",
    "            min_topic_size=2,\n",
    "            nr_topics='auto',\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Fit the model\n",
    "        topics, probs = topic_model.fit_transform(post_texts)\n",
    "        \n",
    "        # Update database with topic information\n",
    "        print(\"\\nUpdating posts with topic information...\")\n",
    "        for i, (post_id, text) in enumerate(zip(post_ids, post_texts)):\n",
    "            topic_id = int(topics[i])\n",
    "            topic_words = topic_model.get_topic(topic_id)\n",
    "            \n",
    "            update_data = {\n",
    "                \"content_topic_id\": topic_id,\n",
    "                \"content_topic_words\": [word for word, _ in topic_words[:5]] if topic_words else [],\n",
    "                \"content_topic_score\": float(probs[i]) if probs is not None else None\n",
    "            }\n",
    "            \n",
    "            db.posts.update_one(\n",
    "                {\"_id\": post_id},\n",
    "                {\"$set\": update_data}\n",
    "            )\n",
    "        \n",
    "        # Print topic analysis\n",
    "        print(\"\\nTopic Analysis Summary:\")\n",
    "        topic_info = topic_model.get_topic_info()\n",
    "        print(topic_info)\n",
    "        \n",
    "        # Print detailed topic information\n",
    "        print(\"\\nDetailed Topics and Example Posts:\")\n",
    "        for topic in topic_info.itertuples():\n",
    "            if topic.Topic != -1:  # Skip outlier topic\n",
    "                print(f\"\\nTopic {topic.Topic} (Count: {topic.Count}):\")\n",
    "                words = [word for word, _ in topic_model.get_topic(topic.Topic)[:5]]\n",
    "                print(\"Keywords:\", \", \".join(words))\n",
    "                \n",
    "                # Find example posts for this topic\n",
    "                example_posts = []\n",
    "                for i, t in enumerate(topics):\n",
    "                    if t == topic.Topic and len(example_posts) < 2:\n",
    "                        example_posts.append(post_texts[i])\n",
    "                \n",
    "                if example_posts:\n",
    "                    print(\"Example content:\")\n",
    "                    for ex in example_posts:\n",
    "                        print(f\"- {ex[:100]}...\")\n",
    "        \n",
    "        # Save topic model visualization\n",
    "        try:\n",
    "            topic_model.visualize_topics()\n",
    "            print(\"\\nTopic visualization has been generated.\")\n",
    "        except Exception as viz_error:\n",
    "            print(f\"Could not generate visualization: {str(viz_error)}\")\n",
    "            \n",
    "    else:\n",
    "        print(\"Not enough posts for meaningful topic modeling\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3645d117",
   "metadata": {},
   "source": [
    "## Video and Photo Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0c7a177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Samsung's media content...\n",
      "\n",
      "=== Content Distribution and Engagement Analysis ===\n",
      "\n",
      "=== Topic Distribution by Content Type ===\n",
      "\n",
      "=== Temporal Posting Patterns ===\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Content Strategy Recommendations ===\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_media_content():\n",
    "    print(\"Analyzing Samsung's media content...\")\n",
    "    \n",
    "    # Get all posts\n",
    "    posts = list(db.posts.find())\n",
    "    \n",
    "    # Initialize containers for analysis\n",
    "    media_stats = {\n",
    "        'video': defaultdict(list),\n",
    "        'sidecar': defaultdict(list),\n",
    "        'image': defaultdict(list)\n",
    "    }\n",
    "    \n",
    "    # Collect data by content type\n",
    "    for post in posts:\n",
    "        content_type = post.get('type', 'unknown')\n",
    "        if content_type in media_stats:\n",
    "            # Engagement metrics\n",
    "            media_stats[content_type]['likes'].append(post.get('likes_count', 0))\n",
    "            media_stats[content_type]['comments'].append(post.get('comments_count', 0))\n",
    "            \n",
    "            # Video specific metrics\n",
    "            if content_type == 'video':\n",
    "                media_stats[content_type]['views'].append(post.get('video_view_count', 0))\n",
    "            \n",
    "            # Topics and sentiment\n",
    "            media_stats[content_type]['topics'].append(post.get('content_topic_words', []))\n",
    "            \n",
    "            # Timestamp for temporal analysis\n",
    "            if post.get('timestamp'):\n",
    "                media_stats[content_type]['timestamps'].append(\n",
    "                    datetime.fromtimestamp(post['timestamp'])\n",
    "                )\n",
    "    \n",
    "    # Generate summary statistics\n",
    "    print(\"\\n=== Content Distribution and Engagement Analysis ===\")\n",
    "    for content_type, stats in media_stats.items():\n",
    "        if stats['likes']:  # Only analyze if we have data\n",
    "            print(f\"\\n{content_type.upper()} CONTENT:\")\n",
    "            print(f\"Count: {len(stats['likes'])}\")\n",
    "            print(f\"Average Likes: {sum(stats['likes']) / len(stats['likes']):,.2f}\")\n",
    "            print(f\"Average Comments: {sum(stats['comments']) / len(stats['comments']):,.2f}\")\n",
    "            \n",
    "            if content_type == 'video' and stats['views']:\n",
    "                print(f\"Average Views: {sum(stats['views']) / len(stats['views']):,.2f}\")\n",
    "                print(f\"Average Engagement Rate: {(sum(stats['likes']) / sum(stats['views']) * 100):,.2f}%\")\n",
    "    \n",
    "    # Topic Analysis by Content Type\n",
    "    print(\"\\n=== Topic Distribution by Content Type ===\")\n",
    "    for content_type, stats in media_stats.items():\n",
    "        if stats['topics']:\n",
    "            topic_words = [word for topic_list in stats['topics'] for word in topic_list]\n",
    "            word_freq = defaultdict(int)\n",
    "            for word in topic_words:\n",
    "                word_freq[word] += 1\n",
    "            \n",
    "            print(f\"\\n{content_type.upper()} Most Common Topics:\")\n",
    "            sorted_topics = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "            for word, freq in sorted_topics[:5]:\n",
    "                print(f\"- {word}: {freq} occurrences\")\n",
    "    \n",
    "    # Temporal Analysis\n",
    "    print(\"\\n=== Temporal Posting Patterns ===\")\n",
    "    for content_type, stats in media_stats.items():\n",
    "        if stats['timestamps']:\n",
    "            timestamps = pd.Series(stats['timestamps'])\n",
    "            print(f\"\\n{content_type.upper()}:\")\n",
    "            print(\"Posting frequency by day of week:\")\n",
    "            day_counts = timestamps.dt.day_name().value_counts()\n",
    "            for day, count in day_counts.items():\n",
    "                print(f\"- {day}: {count} posts\")\n",
    "    \n",
    "    # Generate engagement comparison\n",
    "    try:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Prepare data for plotting\n",
    "        engagement_data = {\n",
    "            'Content Type': [],\n",
    "            'Metric': [],\n",
    "            'Value': []\n",
    "        }\n",
    "        \n",
    "        for content_type, stats in media_stats.items():\n",
    "            if stats['likes']:\n",
    "                # Add likes data\n",
    "                engagement_data['Content Type'].extend([content_type] * len(stats['likes']))\n",
    "                engagement_data['Metric'].extend(['Likes'] * len(stats['likes']))\n",
    "                engagement_data['Value'].extend(stats['likes'])\n",
    "                \n",
    "                # Add comments data\n",
    "                engagement_data['Content Type'].extend([content_type] * len(stats['comments']))\n",
    "                engagement_data['Metric'].extend(['Comments'] * len(stats['comments']))\n",
    "                engagement_data['Value'].extend(stats['comments'])\n",
    "        \n",
    "        # Create DataFrame and plot\n",
    "        df = pd.DataFrame(engagement_data)\n",
    "        sns.boxplot(x='Content Type', y='Value', hue='Metric', data=df)\n",
    "        plt.title('Engagement Distribution by Content Type')\n",
    "        plt.yscale('log')\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate visualization: {str(e)}\")\n",
    "    \n",
    "    # Return the collected stats for further analysis if needed\n",
    "    return media_stats\n",
    "\n",
    "# Run the analysis\n",
    "media_analysis = analyze_media_content()\n",
    "\n",
    "# Additional insights about video content\n",
    "video_stats = media_analysis['video']\n",
    "if video_stats['views']:\n",
    "    print(\"\\n=== Detailed Video Performance Analysis ===\")\n",
    "    \n",
    "    # Calculate view-to-like conversion rate\n",
    "    conversion_rates = [\n",
    "        (likes / views * 100) if views > 0 else 0 \n",
    "        for likes, views in zip(video_stats['likes'], video_stats['views'])\n",
    "    ]\n",
    "    \n",
    "    print(f\"View-to-Like Conversion Rate:\")\n",
    "    print(f\"- Average: {sum(conversion_rates) / len(conversion_rates):.2f}%\")\n",
    "    print(f\"- Highest: {max(conversion_rates):.2f}%\")\n",
    "    print(f\"- Lowest: {min(conversion_rates):.2f}%\")\n",
    "    \n",
    "    # Identify top performing videos\n",
    "    performance_metrics = list(zip(\n",
    "        video_stats['views'],\n",
    "        video_stats['likes'],\n",
    "        video_stats['comments']\n",
    "    ))\n",
    "    \n",
    "    sorted_by_views = sorted(enumerate(performance_metrics), key=lambda x: x[1][0], reverse=True)\n",
    "    \n",
    "    print(\"\\nTop 3 Videos by Views:\")\n",
    "    for i, (idx, (views, likes, comments)) in enumerate(sorted_by_views[:3], 1):\n",
    "        print(f\"{i}. Views: {views:,}, Likes: {likes:,}, Comments: {comments:,}\")\n",
    "\n",
    "# Print recommendations based on the analysis\n",
    "print(\"\\n=== Content Strategy Recommendations ===\")\n",
    "for content_type, stats in media_analysis.items():\n",
    "    if stats['likes']:\n",
    "        avg_engagement = sum(stats['likes']) / len(stats['likes'])\n",
    "        if content_type == 'video':\n",
    "            avg_views = sum(stats['views']) / len(stats['views']) if stats['views'] else 0\n",
    "            print(f\"\\n{content_type.upper()}:\")\n",
    "            print(f\"- Average engagement rate: {(avg_engagement / avg_views * 100 if avg_views > 0 else 0):.2f}%\")\n",
    "            print(\"Recommendation:\", end=\" \")\n",
    "            if avg_views > 0 and (avg_engagement / avg_views) > 0.05:\n",
    "                print(\"Continue investing in video content - showing strong engagement\")\n",
    "            else:\n",
    "                print(\"Consider optimizing video content strategy for better engagement\")\n",
    "        else:\n",
    "            print(f\"\\n{content_type.upper()}:\")\n",
    "            print(f\"- Average likes per post: {avg_engagement:,.2f}\")\n",
    "            print(\"Recommendation:\", end=\" \")\n",
    "            if avg_engagement > 1000:\n",
    "                print(f\"Current {content_type} strategy is effective\")\n",
    "            else:\n",
    "                print(f\"Consider reviewing {content_type} content strategy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
